{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQurD71SwmB+msuRdUZF6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hust-lizewei/MetaFormer/blob/master/tutorial_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ISheB1e6kqh",
        "outputId": "6adcdb3a-f5d4-45dd-8f40-207ded0f4ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "lxVjLrs77A1U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n"
      ],
      "metadata": {
        "id": "LU9womKbCObC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True, padding_side='left')\n",
        "max_token_len = 100\n",
        "input_text = '明月几时有,把酒问青天，不知天上宫阙'\n",
        "output = tokenizer.encode_plus(input_text, max_length=max_token_len, padding=True, return_tensors='pt', truncation=True)\n",
        "\n",
        "print(output['input_ids'].shape, output['attention_mask'].shape)\n",
        "print(f\"******\"*10)\n",
        "\n",
        "# 解码input_ids，不跳过特殊token，不清理token化空格\n",
        "full_tokens = tokenizer.batch_decode(output['input_ids'], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "\n",
        "print(f\"输入Text: {input_text}\")\n",
        "print(f\"分词后结果: {output['input_ids']}\")\n",
        "print(f\"解码后结果: {full_tokens}\")\n",
        "\n",
        "# 打印每个token的解码结果\n",
        "for i in range(len(output['input_ids'][0])):\n",
        "    token = output['input_ids'][0][i].item()  # 使用.item()获取单个值\n",
        "    token_text = tokenizer.convert_ids_to_tokens(token)\n",
        "    print(f\"Token ID: {token}, Token Text: {token_text}, Decoded: {tokenizer.decode([token], skip_special_tokens=False)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nTUWLH07LPq",
        "outputId": "66ce5ff5-5286-4570-83ed-6ffc1605bf59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 17]) torch.Size([1, 17])\n",
            "************************************************************\n",
            "输入Text: 明月几时有,把酒问青天，不知天上宫阙\n",
            "分词后结果: tensor([[ 30858,   9754,  99195,  13343,  18830,     11,  99360,  99525,  56007,\n",
            "          99467,  35727,   3837, 102085, 110154,  99921, 119082, 151643]])\n",
            "解码后结果: ['明月几时有,把酒问青天，不知天上宫阙<|endoftext|>']\n",
            "Token ID: 30858, Token Text: æĺİ, Decoded: 明\n",
            "Token ID: 9754, Token Text: æľĪ, Decoded: 月\n",
            "Token ID: 99195, Token Text: åĩł, Decoded: 几\n",
            "Token ID: 13343, Token Text: æĹ¶, Decoded: 时\n",
            "Token ID: 18830, Token Text: æľī, Decoded: 有\n",
            "Token ID: 11, Token Text: ,, Decoded: ,\n",
            "Token ID: 99360, Token Text: æĬĬ, Decoded: 把\n",
            "Token ID: 99525, Token Text: éħĴ, Decoded: 酒\n",
            "Token ID: 56007, Token Text: éĹ®, Decoded: 问\n",
            "Token ID: 99467, Token Text: éĿĴ, Decoded: 青\n",
            "Token ID: 35727, Token Text: å¤©, Decoded: 天\n",
            "Token ID: 3837, Token Text: ï¼Į, Decoded: ，\n",
            "Token ID: 102085, Token Text: ä¸įçŁ¥, Decoded: 不知\n",
            "Token ID: 110154, Token Text: å¤©ä¸Ĭ, Decoded: 天上\n",
            "Token ID: 99921, Token Text: å®«, Decoded: 宫\n",
            "Token ID: 119082, Token Text: éĺĻ, Decoded: 阙\n",
            "Token ID: 151643, Token Text: <|endoftext|>, Decoded: <|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  \"\"\"    因果注意力    \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    heads,  # 自注意力的头数\n",
        "    head_dim,  # 单头embed_size\n",
        "    embed_dim,  # hidden_size\n",
        "    dropout=0.0,\n",
        "    ):\n",
        "    super(CausalAttention, self).__init__()\n",
        "    self.heads = heads\n",
        "    self.head_dim = head_dim\n",
        "    self.emb_dim = embed_dim\n",
        "    assert embed_dim == head_dim * heads\n",
        "    self.dropout = dropout\n",
        "    # 通过wordEmbedding-->扩展得到QKV矩阵(N, 1, embed_dim) -> (N, 1 3*emb_dim)\n",
        "    self.qkv_projection = nn.Linear(self.emb_dim, 3*self.emb_dim)\n",
        "    self.dropout1 = nn.Dropout(self.dropout)\n",
        "    self.dropout2 = nn.Dropout(self.dropout)\n",
        "    self.ffn = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "  def forward(self, hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    :param hidden_states: (N, seqLen, embed_dim), Context hidden_state\n",
        "    :param attention_mask: (N, seqLen) --> (N, 1, seqLen, seqLen), 0: valid,\n",
        "    :return: attention之后的hidden_state        \"\"\"\n",
        "    bs, seq_len, _ = hidden_states.size()\n",
        "    qkv = self.qkv_projection(hidden_states)\n",
        "    # 每个QKV的形状为 (N, seqLen, embed_dim=heads*head_dim)\n",
        "    q, k, v = torch.split(qkv, qkv.size(-1)//3, dim=-1)\n",
        "    # 将q,k,v划分为多头 (N, seqLen, head_dim)  --> (N, seqLen, heads, head_dim) --> (N, heads, seqLen, head_dim)\n",
        "    q = q.view(bs, seq_len, self.heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "    k = k.view(bs, seq_len, self.heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "    v = v.view(bs, seq_len, self.heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "    # 注意力计算-点积注意力: Attention = softmax[(Q*K')/sqrt(d)]*V\n",
        "    # QK': (N, heads, seqLen, head_dim) * (N, heads, head_dim, seqLen) --> (N, heads, seqLen, seqLen)\n",
        "    #      (N, heads, i, seqLen): 表示位置为i的token对所有的seqLen个token的注意力\n",
        "    logits = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.heads)\n",
        "    logits = logits.to(torch.float32)\n",
        "    # 构造上三角阵形式的mask，实现causal效果: 等价于将对于位置i的token而言将位置i+1之后的所有logits全部置为无穷小\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.long)).to(logits.device)\n",
        "    # ******** mask.shape=torch.Size([208, 208])|torch.int64, logits.shape=torch.Size([32, 8, 208, 208])|torch.float16\n",
        "    # print(f\"******** mask.shape={mask.shape}|{mask.dtype}, logits.shape={logits.shape}|{logits.dtype}\")\n",
        "    logits = logits.masked_fill(mask == 0, -1e19)\n",
        "    # print(f\"============= logits.shape={logits.shape}||{attention_mask.shape}\")\n",
        "    # logits = logits.masked_fill(attention_mask == 0, -1e12)\n",
        "    logits = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    logits = self.dropout1(logits)\n",
        "    # (N, heads, seqLen) *  (N, heads, seqLen, head_dim) --> (N, heads, seqLen, head_dim) --> (N, seqLen, embed_dim)\n",
        "    weighted_v = torch.matmul(logits, v).transpose(1, 2).contiguous().view(bs, seq_len, -1).contiguous()\n",
        "    weighted_v = self.ffn(weighted_v)\n",
        "    weighted_v = self.dropout2(weighted_v)\n",
        "    return weighted_v"
      ],
      "metadata": {
        "id": "M3Q4HMbmCkWT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  解码层: 对于当前位置为i的token，对0-i范围的context进行编码，生成i+1及之后新token的预测概率 --> 自回归生成过程\n",
        "  Note: 最终层需要生成预测概率，但中间层则只需要做CasualAttention生成表示即可\n",
        "  结构:Input_X-> LN -> CausalAttention-> LN-> FFN-> Activation\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    heads,  # 自注意力的头数\n",
        "    head_dim,  # 单头embed_size\n",
        "    embed_dim,  # hidden_size\n",
        "    ffn_expand=3,  # FFN中上采样比例\n",
        "    ffn_dropout=0.0,\n",
        "    attn_dropout=0.0,\n",
        "    max_seq_len=512\n",
        "  ):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.ffn_expand = ffn_expand\n",
        "    self.ffn_dropout = ffn_dropout\n",
        "    self.ln1 = nn.LayerNorm(self.embed_dim)\n",
        "    self.ln2 = nn.LayerNorm(self.embed_dim)\n",
        "    self.casual_attention = CausalAttention(\n",
        "        heads,\n",
        "        head_dim,\n",
        "        embed_dim,\n",
        "        attn_dropout\n",
        "    )\n",
        "    self.act = nn.GELU()\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(embed_dim, self.ffn_expand * embed_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(self.ffn_expand * embed_dim, embed_dim),\n",
        "        nn.Dropout(self.ffn_dropout))\n",
        "\n",
        "\n",
        "  def forward(self, x, attention_mask):\n",
        "    \"\"\"\n",
        "    :param x: HiddenState (N, seqLen, emb_dim)\n",
        "    :return: HiddenState (N, seqLen, emb_dim)\n",
        "    \"\"\"\n",
        "    x1 = self.ln1(x)\n",
        "    x1 = self.casual_attention(x1, attention_mask)\n",
        "    x = x1 + x\n",
        "    x2 = self.ffn(x)\n",
        "    x = x2 + x\n",
        "    x = self.act(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "200z5UpSDFgl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ChatLLM(nn.Module):\n",
        "  \"\"\"\n",
        "  模型结构: 采用DecodeOnly结构，直接采用多层解码器堆叠即可,并封装: 推理预测功能\n",
        "  self.tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    heads,  # 自注意力的头数\n",
        "    head_dim,  # 单头embed_size\n",
        "    embed_dim,  # hidden_size\n",
        "    ffn_expand=4,  # FFN中上采样比例\n",
        "    ffn_dropout=0.0,\n",
        "    attn_dropout=0.0,\n",
        "    decoder_layers=4,\n",
        "    max_seq_len=512,\n",
        "    vocab_size=-1  # 词库规模\n",
        "    ):\n",
        "    super(ChatLLM, self).__init__()\n",
        "    self.heads = heads\n",
        "    self.head_dim = head_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.ffn_expand = ffn_expand\n",
        "    self.ffn_dropout = ffn_dropout\n",
        "    self.attn_dropout = attn_dropout\n",
        "    self.decoder_layers = decoder_layers\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.vocab_size = vocab_size\n",
        "    self.token_embedding = torch.nn.Embedding(vocab_size, embedding_dim=embed_dim)\n",
        "    self.position_embedding = torch.nn.Embedding(max_seq_len, embedding_dim=embed_dim)\n",
        "    self.decoders = nn.Sequential(\n",
        "        *[DecoderLayer(heads, head_dim, embed_dim, ffn_expand, ffn_dropout, attn_dropout, max_seq_len) for _ in range(decoder_layers)]\n",
        "        )\n",
        "    # 自回归生成head\n",
        "    self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "    self.temp = nn.Parameter(torch.Tensor([0.1]))\n",
        "    self.apply(init_weights)\n",
        "\n",
        "    def from_pretrain(self, pretrain):\n",
        "      state_dict = torch.load(pretrain, map_location='cpu')\n",
        "      self.load_state_dict(state_dict, strict=True)\n",
        "      self.to(self.device)\n",
        "      return self\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "      if torch.cuda.is_available():\n",
        "          return torch.device('cuda')\n",
        "      return torch.device('cpu')\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "      \"\"\"\n",
        "      :param input_ids: (N, seqLen)\n",
        "      :param attention_mask: (N, seqLen)\n",
        "      :param labels: 未进行了label_shift\n",
        "      :return:\n",
        "      \"\"\"\n",
        "      # (N, seqLen, embed_dim)\n",
        "      # print(f\"======**** vocab_size={self.vocab_size}, input_ids.device={input_ids.shape}, device={input_ids.shape}||token_embedding.device={self.token_embedding.weight.shape}||position_embedding.device={self.position_embedding.weight.shape}\")\n",
        "      # raise NotImplementedError\n",
        "      token_embedding = self.token_embedding(input_ids)\n",
        "      position_idx = torch.arange(0, input_ids.size(1), dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "      position_embedding = self.position_embedding(position_idx)\n",
        "      x = token_embedding + position_embedding\n",
        "      for decoder in self.decoders:\n",
        "          x = decoder(x, attention_mask)\n",
        "      logits = self.lm_head(x) / self.temp  # (N, seqLen, vocab_size)\n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "          assert labels.size(-1) == input_ids.size(-1)\n",
        "          labels = labels.to(logits.device)\n",
        "          shifted_logits = logits[:, :-1, :].contiguous()  # [0, 1, 2, ...,n-1]\n",
        "          shifted_labels = labels[:, 1:].contiguous()  # [1, 2, 3, ...,n]\n",
        "          shifted_logits = shifted_logits.view(-1, self.vocab_size)\n",
        "          shifted_labels = shifted_labels.view(-1)\n",
        "          loss = torch.nn.functional.cross_entropy(shifted_logits, shifted_labels)\n",
        "      return logits, loss\n"
      ],
      "metadata": {
        "id": "URIVKNxfDw1Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def generate(self, input_ids, attention_mask, max_generate_tokens=100, temperature=1.0, **kwargs):\n",
        "  \"\"\"\n",
        "  生成过程计聊天的回答过程, 将进行逐个token生成，直到出现EOS或者达到最大长度限制为止\n",
        "  :param input_ids:\n",
        "  :param attention_mask:\n",
        "  :param max_generate_tokens:\n",
        "  :param temperature::return: (N, generated_token_nums)\n",
        "  \"\"\"\n",
        "  top_k = 8\n",
        "  full_token_id = input_ids[..., :self.max_seq_len]\n",
        "  attention_mask = attention_mask[..., :self.max_seq_len]\n",
        "  for _ in range(max_generate_tokens):\n",
        "    logits, _ = self.forward(full_token_id, attention_mask, None)\n",
        "    next_token_logits = logits[:, -1, :] / temperature  # (N, vocab_size)\n",
        "    tops, _ = torch.topk(next_token_logits, 500, dim=-1)\n",
        "    next_token_logits = next_token_logits.masked_fill(next_token_logits < tops[:, [-1]], -1e9)\n",
        "    # (N, vocab_size)\n",
        "    next_token_logits = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
        "    # 获取topK的nex_token作为候选池, 并进行采样\n",
        "    tops, _ = torch.topk(next_token_logits, top_k, dim=-1)\n",
        "    next_token_logits = next_token_logits.masked_fill(next_token_logits < tops[:, [-1]], 0)\n",
        "    # (N, 1)\n",
        "    next_token_id = torch.multinomial(next_token_logits, 1)\n",
        "    full_token_id = torch.concat([full_token_id, next_token_id], dim=-1)\n",
        "  return full_token_id"
      ],
      "metadata": {
        "id": "E8J3SbJ0Ebc1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeZXjhHsEtY-",
        "outputId": "d9e56324-6f77-4d8b-db47-21b46cc0349c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "from omegaconf import OmegaConf\n",
        "from trainer import MyTrainer\n",
        "from models.chat import ChatLLM\n",
        "from models.tokenizer import tokenizer\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "cfg_path = 'models/config.yaml'\n",
        "conf = OmegaConf.load(cfg_path)\n",
        "model_config = OmegaConf.to_container(conf.trainer_config.model_config)\n",
        "model = ChatLLM(**model_config).to('cuda')\n",
        "model.eval()\n",
        "input_text = '诗词题目：独坐窗台'\n",
        "full_tokens = model.chat(input_text, tokenizer, max_generate_tokens=50, temperature=0.8)\n",
        "print(full_tokens)\n"
      ],
      "metadata": {
        "id": "DLpFBbpgEmdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}